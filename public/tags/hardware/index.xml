<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hardware on The Patch Panel</title>
        <link>http://192.168.100.63:1313/tags/hardware/</link>
        <description>Recent content in Hardware on The Patch Panel</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Thu, 15 May 2025 14:30:00 -0400</lastBuildDate><atom:link href="http://192.168.100.63:1313/tags/hardware/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>First Homelab: Reflections, Diagram, and Stack</title>
        <link>http://192.168.100.63:1313/projects/homelab/</link>
        <pubDate>Thu, 01 May 2025 15:00:00 -0400</pubDate>
        
        <guid>http://192.168.100.63:1313/projects/homelab/</guid>
        <description>&lt;img src="http://192.168.100.63:1313/img/HL.jpg" alt="Featured image of post First Homelab: Reflections, Diagram, and Stack" /&gt;&lt;p&gt;&lt;img src=&#34;http://192.168.100.63:1313/img/HL.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Homelab Diagram&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;building-my-first-homelab-reflections-stack-and-next-steps&#34;&gt;&lt;a href=&#34;#building-my-first-homelab-reflections-stack-and-next-steps&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;Building My First Homelab: Reflections, Stack, and Next Steps
&lt;/h1&gt;&lt;p&gt;I&amp;rsquo;ve always admired the impressive homelab diagrams circulating on self-hosting forums. After years of lurking, tinkering, and gradually collecting hardware, I finally decided to document my own setup and journey.&lt;/p&gt;
&lt;p&gt;This post summarizes what I&amp;rsquo;m running, what I&amp;rsquo;ve learned along the way, and how a casual experiment turned into a full-blown hobby.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;from-proxmox-curiosity-to-a-full-rack&#34;&gt;&lt;a href=&#34;#from-proxmox-curiosity-to-a-full-rack&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;From Proxmox Curiosity to a Full Rack
&lt;/h2&gt;&lt;p&gt;The homelab journey began a couple of years ago, when I picked up a Lenovo m720q Tiny to run a Kali VM for some light pen-testing and CTF challenges. The spark was there, but life intervened, and the project was shelved.&lt;/p&gt;
&lt;p&gt;Still, the curiosity stuck. After discovering the homelab and self-hosted communities, I took the plunge and bought two used servers—a Dell r730xd and an r430—right before the arrival of our second child. As you might expect, the servers went straight into storage.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;picking-up-the-project&#34;&gt;&lt;a href=&#34;#picking-up-the-project&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;Picking Up the Project
&lt;/h2&gt;&lt;p&gt;Earlier this year, with a bit more time and sleep, I finally assembled the rack, wired up switches, and started building out the network. What started as a few VMs quickly became a full stack: network segmentation, VLANs, containers, and a host of new services.&lt;/p&gt;
&lt;p&gt;Recently, my attention has shifted toward AI experimentation. I built a dedicated AI rig, first with MI50 GPUs and now with 3090s and 3090Tis. Running local LLMs, experimenting with AI agents, and integrating custom tools has been a huge part of the learning curve.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;background&#34;&gt;&lt;a href=&#34;#background&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;Background
&lt;/h2&gt;&lt;p&gt;For context, I’m not a professional system administrator. My background is in tech leadership and data science for a large company, but the homelab is strictly a hobby. It’s a late-night project fueled by curiosity, trial-and-error, and a desire to truly understand the technology I use.&lt;/p&gt;
&lt;p&gt;I enjoy exploring open-source services, experimenting with network architectures (sometimes to excess), and continually learning about security and automation in a hands-on way.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;current-stack&#34;&gt;&lt;a href=&#34;#current-stack&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;Current Stack
&lt;/h2&gt;&lt;p&gt;Here&amp;rsquo;s what I have running today, organized by function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Media &amp;amp; Entertainment&lt;/strong&gt;&lt;br&gt;
Jellyfin, *arr suite (Readarr, Prowlarr, etc.), qBittorrent with Gluetun VPN, Audiobookshelf&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lifestyle &amp;amp; Smart Home&lt;/strong&gt;&lt;br&gt;
Tandoor (recipes), Bar Assistant, Plant It, FreshRSS, Home Assistant&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Productivity &amp;amp; Documentation&lt;/strong&gt;&lt;br&gt;
Gitea (private Git), Nextcloud, PaperlessNGX, Draw.io, Filebrowser, n8n, Karakeep, Linkwarden, SANE Network Scanning, Kopia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Databases&lt;/strong&gt;&lt;br&gt;
MariaDB, PostgreSQL, InfluxDB&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Monitoring &amp;amp; Management&lt;/strong&gt;&lt;br&gt;
Grafana, Uptime Kuma, Homepage dashboard, Portainer, Watchtower, Prometheus&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Security &amp;amp; Networking&lt;/strong&gt;&lt;br&gt;
OPNSense, Fail2Ban, Authelia (SSO/2FA), Pi-hole, Traefik, MITMproxy, Tailscale, Cloudflared&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI Stack&lt;/strong&gt;&lt;br&gt;
llama.cpp, AnythingLLM, PostgreSQL with pgvector, n8n for orchestration&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;upcoming-projects&#34;&gt;&lt;a href=&#34;#upcoming-projects&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;Upcoming Projects
&lt;/h2&gt;&lt;p&gt;Like most homelabbers, my &amp;ldquo;future plans&amp;rdquo; list is always growing. On the shortlist:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Changedetection, Dashy, Glance, Homarr&lt;/li&gt;
&lt;li&gt;Revisit Matrix/Element setup&lt;/li&gt;
&lt;li&gt;Firefly III (personal finance), Immich (photos), Joplin (notes)&lt;/li&gt;
&lt;li&gt;Lube Logger, Monica, OnlyOffice, Open Meteo, Rocket.Chat, Syncthing, VSCode Server (currently running locally)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lessons-learned&#34;&gt;&lt;a href=&#34;#lessons-learned&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;Lessons Learned
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Hardware is interesting, but the real power is in what you do with it. The software ecosystem is where experimentation pays off.&lt;/li&gt;
&lt;li&gt;Network segmentation can be addictive. One VLAN for IoT leads to many more as complexity grows.&lt;/li&gt;
&lt;li&gt;Backups matter. Tools like Kopia and PaperlessNGX have already proven invaluable for recovery and organization.&lt;/li&gt;
&lt;li&gt;You do not need to be a sysadmin to run a homelab. Curiosity and persistence are more important than credentials.&lt;/li&gt;
&lt;li&gt;There is always more to learn, and that&amp;rsquo;s what makes this hobby worth the investment.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;&lt;a href=&#34;#next-steps&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;Next Steps
&lt;/h2&gt;&lt;p&gt;I&amp;rsquo;m planning a follow-up with hardware photos and a deep dive into the AI stack and automation workflows. If you have feedback, advice, or want to trade configs, let me know. I&amp;rsquo;m always looking for ways to improve or expand the lab.&lt;/p&gt;
&lt;p&gt;Stay tuned for more, and happy self-hosting.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>The Efficiency Revolution: When Smaller AI Models Start Outpunching Their Weight Class</title>
        <link>http://192.168.100.63:1313/musings/gemma3-ai/</link>
        <pubDate>Thu, 15 May 2025 14:30:00 -0400</pubDate>
        
        <guid>http://192.168.100.63:1313/musings/gemma3-ai/</guid>
        <description>&lt;img src="http://192.168.100.63:1313/img/head/gemma.png" alt="Featured image of post The Efficiency Revolution: When Smaller AI Models Start Outpunching Their Weight Class" /&gt;&lt;hr&gt;
&lt;blockquote&gt;
&lt;h4 id=&#34;ai-summary&#34;&gt;&lt;a href=&#34;#ai-summary&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;&lt;strong&gt;AI Summary&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Small, efficient AI models like &lt;a class=&#34;link&#34; href=&#34;https://ai.google.dev/gemma/docs&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Google&amp;rsquo;s Gemma&lt;/a&gt; and &lt;a class=&#34;link&#34; href=&#34;https://github.com/deepseek-ai/DeepSeek-Coder-V2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DeepSeek-Coder-V2&lt;/a&gt; are delivering performance that rivals much larger systems.&lt;/li&gt;
&lt;li&gt;This efficiency breakthrough dramatically lowers the barrier to entry for AI development, making powerful models accessible to smaller teams and individual developers.&lt;/li&gt;
&lt;li&gt;The shift could trigger a fundamental change in AI competition—from raw parameter count to performance-per-watt optimization.&lt;/li&gt;
&lt;li&gt;Real-world adoption and production reliability will ultimately determine if this trend reshapes the entire AI landscape.&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-david-vs-goliath-moment-weve-been-waiting-for&#34;&gt;&lt;a href=&#34;#the-david-vs-goliath-moment-weve-been-waiting-for&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;The David vs. Goliath Moment We&amp;rsquo;ve Been Waiting For
&lt;/h3&gt;&lt;p&gt;An industry whisper has crystallized into something undeniable: the era of &amp;ldquo;bigger is always better&amp;rdquo; for AI is hitting a wall. The latest wave of efficient models isn&amp;rsquo;t just incrementally better—they&amp;rsquo;re rewriting the rules entirely.&lt;/p&gt;
&lt;p&gt;Take &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/deepseek-ai/DeepSeek-Coder-V2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;DeepSeek-Coder-V2&lt;/a&gt;&lt;/strong&gt;, which I&amp;rsquo;ve been running locally. This thing has 236 billion total parameters but only activates 21 billion during inference thanks to its &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1701.06538&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Mixture-of-Experts (MoE)&lt;/a&gt; architecture. It&amp;rsquo;s a genuinely impressive piece of engineering that delivers serious coding capabilities without requiring a small power plant to run.&lt;/p&gt;
&lt;p&gt;Then Google drops &lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://ai.google.dev/gemma/docs/core&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Gemma 3&lt;/a&gt;&lt;/strong&gt;, and suddenly the conversation shifts. Here&amp;rsquo;s a (up to) 27-billion parameter model that&amp;rsquo;s going toe-to-toe with systems many times its size. On benchmarks like MATH, it&amp;rsquo;s not just competitive—it&amp;rsquo;s demonstrating that architectural cleverness can trump raw scale.&lt;/p&gt;
&lt;p&gt;This isn&amp;rsquo;t about a compact car somehow beating a semi-truck. It&amp;rsquo;s about discovering that the race we thought we were running might have been the wrong race entirely.&lt;/p&gt;
&lt;h3 id=&#34;why-this-breakthrough-actually-changes-everything&#34;&gt;&lt;a href=&#34;#why-this-breakthrough-actually-changes-everything&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;Why This Breakthrough Actually Changes Everything
&lt;/h3&gt;&lt;h4 id=&#34;the-economics-are-about-to-flip&#34;&gt;&lt;a href=&#34;#the-economics-are-about-to-flip&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;The Economics Are About to Flip
&lt;/h4&gt;&lt;p&gt;Until now, serious AI development meant accepting staggering costs. Training something like GPT-4 reportedly cost &lt;a class=&#34;link&#34; href=&#34;https://www.visualcapitalist.com/the-surging-cost-of-training-ai-models/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;over $78 million&lt;/a&gt;, and that&amp;rsquo;s before you factor in the operational nightmare of keeping it running on clusters of high-end hardware.&lt;/p&gt;
&lt;p&gt;This created an obvious problem: only a handful of companies could afford to play at the cutting edge. The rest of us were relegated to using their APIs and hoping their priorities aligned with ours.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The promise of efficient models isn&amp;rsquo;t just better performance-per-dollar—it&amp;rsquo;s about fundamentally changing who gets to participate in AI development.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;DeepSeek-Coder runs beautifully on a single consumer GPU. Gemma 2 9B can deliver impressive results on hardware that&amp;rsquo;s within reach of university labs, smaller companies, and individual developers who know what they&amp;rsquo;re doing.&lt;/p&gt;
&lt;h4 id=&#34;the-environmental-reality-check&#34;&gt;&lt;a href=&#34;#the-environmental-reality-check&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;The Environmental Reality Check
&lt;/h4&gt;&lt;p&gt;The energy consumption story around AI has been getting uncomfortable. Data centers dedicated to AI are on track to consume electricity &lt;a class=&#34;link&#34; href=&#34;https://www.reuters.com/business/energy/us-utilities-grapple-with-big-techs-massive-power-demands-data-centers-2025-04-07/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;equivalent to entire countries&lt;/a&gt;. When you&amp;rsquo;re running models locally and seeing what&amp;rsquo;s possible with a fraction of the power draw, the wastefulness of the current approach becomes obvious.&lt;/p&gt;
&lt;p&gt;Achieving comparable performance with dramatically less hardware isn&amp;rsquo;t just cost-effective—it&amp;rsquo;s the only sustainable path forward. The alternative is an AI industry that burns through resources at an unconscionable rate.&lt;/p&gt;
&lt;h3 id=&#34;what-gets-unlocked&#34;&gt;&lt;a href=&#34;#what-gets-unlocked&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;What Gets Unlocked
&lt;/h3&gt;&lt;p&gt;The democratization effect here could be profound:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smaller companies can finally compete instead of being permanently relegated to API consumers.&lt;/li&gt;
&lt;li&gt;Individual developers gain access to state-of-the-art capabilities on their own hardware.&lt;/li&gt;
&lt;li&gt;Global innovation becomes possible for institutions that couldn&amp;rsquo;t previously justify the infrastructure costs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The real excitement isn&amp;rsquo;t just technical—it&amp;rsquo;s about what happens when powerful AI tools escape the confines of big tech and start showing up in unexpected places.&lt;/p&gt;
&lt;h3 id=&#34;the-necessary-skepticism&#34;&gt;&lt;a href=&#34;#the-necessary-skepticism&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;The Necessary Skepticism
&lt;/h3&gt;&lt;p&gt;Impressive benchmark numbers don&amp;rsquo;t automatically translate to production reliability. The gap between a model performing well on a benchmark like HumanEval and actually being useful for day-to-day development work can be significant.&lt;/p&gt;
&lt;p&gt;The integration challenge is real too. Adopting new model architectures requires developers to adapt workflows, learn new tools, and solve compatibility issues. Technical superiority doesn&amp;rsquo;t guarantee adoption.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The true test isn&amp;rsquo;t whether these models can hit impressive numbers on standardized benchmarks—it&amp;rsquo;s whether they can deliver consistent value in messy, real-world applications.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The success of these efficient models will ultimately depend on community adoption. The open-source nature of both Gemma and DeepSeek is crucial here—it enables the kind of collaborative ecosystem that made Linux successful.&lt;/p&gt;
&lt;h3 id=&#34;the-bigger-transformation&#34;&gt;&lt;a href=&#34;#the-bigger-transformation&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;The Bigger Transformation
&lt;/h3&gt;&lt;p&gt;If this efficiency trend holds, we&amp;rsquo;re looking at a fundamental shift in how the AI industry operates.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Competition is about to get more interesting.&lt;/strong&gt; The focus could pivot from brute-force parameter scaling to sophisticated optimization. Performance-per-watt and performance-per-dollar become the metrics that matter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hardware demand patterns will change.&lt;/strong&gt; The insatiable appetite for ever-larger GPU clusters might give way to demand for more specialized, efficient processors. This could reshape entire market dynamics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open source gains serious leverage.&lt;/strong&gt; When powerful, efficient models are freely available, the value proposition of closed, proprietary systems becomes harder to justify.&lt;/p&gt;
&lt;h3 id=&#34;what-to-watch&#34;&gt;&lt;a href=&#34;#what-to-watch&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;What to Watch
&lt;/h3&gt;&lt;p&gt;The next few months will reveal whether this is a lasting transformation or just an interesting moment.&lt;/p&gt;
&lt;p&gt;Keep an eye on independent validation from researchers who aren&amp;rsquo;t affiliated with the model creators. Track adoption metrics—how quickly do these efficient models get incorporated into major projects and commercial applications?&lt;/p&gt;
&lt;p&gt;Most importantly, watch how the major players respond. Will they double down on scale or pivot toward efficiency? The competitive response will tell us everything about where this is heading.&lt;/p&gt;
&lt;h3 id=&#34;the-core-insight&#34;&gt;&lt;a href=&#34;#the-core-insight&#34; class=&#34;header-anchor&#34;&gt;&lt;/a&gt;The Core Insight
&lt;/h3&gt;&lt;p&gt;The raw power of massive AI models is undeniably impressive. But the real revolution might come from making that power accessible to everyone who has something interesting to build with it.&lt;/p&gt;
&lt;p&gt;If models like Gemma and DeepSeek have genuinely cracked the code on delivering premium AI performance at an economical price point, they&amp;rsquo;re not just advancing the technology—they&amp;rsquo;re opening the door for a more diverse, innovative, and sustainable AI future.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s the kind of shift that changes everything.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
